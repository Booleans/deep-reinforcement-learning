{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 02 - OpenAI Gym\n",
    "\n",
    "We will define an environment that will give the agent random rewards for a limited number of steps, regardless of the agent's actions. This scenario is not very useful, but it will allow us to focus on specific methods in both the environment and agent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from typing import List\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.steps_left = 10\n",
    "    \n",
    "    def get_observation(self) -> List[float]:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    \n",
    "    def get_actions(self) -> List[int]:\n",
    "        return [0, 1]\n",
    "    \n",
    "    def is_done(self) -> bool:\n",
    "        return self.steps_left == 0\n",
    "    \n",
    "    def action(self, action) -> float:\n",
    "        if self.is_done():\n",
    "            raise Exception('Game is over.')\n",
    "        self.steps_left -= 1\n",
    "        return random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_observation()` method is supposed to return the current environment's observation to the agent. It is usually implemented as some function of the internal state of the environment. \n",
    "\n",
    "The `get_actions()` method allows the agent to query the set of actions it can execute. Normally, the set of actions that the agent can execute does not change over time, but some actions can become impossible in different states. In this simplistic example, there are only two actions that the agent can carry out, which are encoded as 0 and 1.\n",
    "\n",
    "The `action()` method is the central piece in the environment's functionality. It does two things - handles the agent's action and returns the reward for this action. In our example, the reward is random and its action is discarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "    def step(self, env):\n",
    "        current_obs = env.get_observation()\n",
    "        actions = env.get_actions()\n",
    "        reward = env.action(random.choice(actions))\n",
    "        self.total_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `step` function accepts the environment instance as an argument and allows the agent to perform the following actions:\n",
    "\n",
    "* Observe the environment.\n",
    "* Make a decision about the action to take based on the observations. \n",
    "* Submit the action to the environment.\n",
    "* Get the reward for the current step.\n",
    "\n",
    "Our agent is dull and ignores the observations obtained during the decision-making process about which action to take. Instead, every action is selected randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 4.7782\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = Environment()\n",
    "    agent = Agent()\n",
    "    \n",
    "    while not env.is_done():\n",
    "        agent.step(env)\n",
    "        \n",
    "    print(f'Total Reward: {agent.total_reward:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CartPole environment.\n",
    "e = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation of this environment is four floating-point numbers containing\n",
    "information about the x coordinate of the stick's center of mass, its speed, its angle to\n",
    "the platform, and its angular speed. Of course, by applying some math and physics\n",
    "knowledge, it won't be complicated to convert these numbers into actions when\n",
    "we need to balance the stick, but our problem is this â€“ how do we learn to balance\n",
    "this system without knowing the exact meaning of the observed numbers and only\n",
    "by getting the reward? The reward in this environment is 1, and it is given on every\n",
    "time step. The episode continues until the stick falls, so to get a more accumulated\n",
    "reward, we need to balance the platform in a way to avoid the stick falling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0042278 , -0.00807371,  0.03678896, -0.02680858])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = e.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset the environment and obtained the first observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space field is of the Discrete type, so our actions will be just 0 or 1, where 0 means pushing the platform to the left and 1 means to the right. The observation space is of Box(4,), which means a vector of size 4 with the values inside the `[-inf, inf]` interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00406632, -0.2037034 ,  0.03625279,  0.27725092]), 1.0, False, {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pushed the platform to the left by executing the action 0 and for the tuple of four elements:\n",
    "\n",
    "* A new observation which is a new vector of four numbers.\n",
    "* A reward of 1.0.\n",
    "* The `done` flag with value False, which means the episdoe is not over yet.\n",
    "* Extra information about the environment, which is an empty dictionary.\n",
    "\n",
    "Next we will use the `sample()` method of the `Space` class on the `action_space` and `observation_space`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6325544e+00, -2.9759798e+38, -2.2532640e-01,  3.1070276e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.8199971e-01,  1.4081275e+38,  4.0631613e-01, -7.2184949e+37],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample from the action space could be used when you're not sure how to perform an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Random CartPole Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ended in 38 steps, total reward 38.000000.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v0')\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    print(f'Episode ended in {total_steps} steps, total reward {total_reward:2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Gym Functionality - Wrappers and Monitors\n",
    "\n",
    "#### Wrappers\n",
    "\n",
    "There are many situations where you want to \"wrap\" the existing environment and add some extra logic for doing something. Gym provides a convenient framework for these situations - the `Wrapper` class.\n",
    "\n",
    "There are subclasses of `Wrapper` that allow the filtering of only a specific portion of information.\n",
    "\n",
    "* `ObservationWrapper`\n",
    "* `RewardWrapper`\n",
    "* `ActionWrapper`\n",
    "\n",
    "Let's imagine a situation where we want to interview in the stream of actions sent by the agent and, with a probability of 10%, replace the current action with a random one. By issuing random actions, we make our agent explore the environment and from time to time drift away from the beaten track of its policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from typing import TypeVar\n",
    "import random\n",
    "\n",
    "Action = TypeVar('Action')\n",
    "\n",
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, epsilon=0.1):\n",
    "        super(RandomActionWrapper, self).__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def action(self, action:Action) -> Action:\n",
    "        if random.random() < self.epsilon:\n",
    "            print('Random!')\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we initialized our wrapper by calling a parent's `__init__` method and saving epsilon. \n",
    "\n",
    "Now it's time to apply our wrapper. We will create a normal CartPole environment and pass it to our `Wrapper` constructor. From here on, we will use our wrapper as a normal `Env` instance, instead of the original CartPole. As the `Wrapper` class inherits the `Env` class and exposes the same interface, we can nest our wrappers in any combination we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 10.000000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = RandomActionWrapper(gym.make('CartPole-v0'))\n",
    "    \n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    while True:\n",
    "        obs, reward, done, _ = env.step(0)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    print(f'Reward: {total_reward:2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
